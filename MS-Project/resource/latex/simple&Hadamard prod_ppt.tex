\documentclass{beamer}             %文档类型用beamer,最好用的幻灯片制作宏包之一
%\hypersetup{CJKbookmarks=true}     %用于实现超链接
%\usepackage{ctex}                  %用于编辑中文文档的宏包
\usepackage[english]{babel}
\usepackage{graphicx}
\usetheme[secheader]{Madrid}       %使得幻灯片美观实用的beamer宏包的主题之一
%\usecolortheme{rose}              %打印稿用浅颜色好些,这是beamer宏包的颜色主题之一
%\usecolortheme{seahorse}          %打印稿用浅颜色好些,这是beamer宏包的颜色主题之一
\begin{document}                   %文档开始
\title{Matrix Derivatives with Chain Rule and Rules for Simple, Hadamard, and Kronecker Products (1) }           %标题
\author{Mubing Zhou, Rongpeng Li, Wenchao Zhang}          %作者
\institute{SUSTC}       %单位
%\date{2013-5-1}
\begin{frame}[plain]               %“帧”开始
  \titlepage                       %标题页-封面
\end{frame}                        %“帧”结束
%\heiti\huge                        %正文字体字号
\section{Chain Rule and Product Rule}
\begin{frame}
\frametitle{Notations and Definitions}
\begin{itemize}
 % \item $I^n$: $n\times n$ identity matrix.
  \item ${\overline X}: 1\times nm $ row vector of all elements of an $n\times m$ matrix $X$, taken row by row.
  \item $D_{{\overline X}}: nm\times nm$ diagonal matrix with diagonal elements $d_{ii} = x_i$, the corresponding element of the row vector ${\overline X}$.
  \item $X*Y$: Hadamard product of two $n\times m$ matrices $X$ and $Y$, i.e $X*Y = [x_{ij}y_{ij}]$.
  \item $X\otimes Y$: Kronecker product of two matrices, i.e., $X\otimes Y = [x_{ij}Y]$
\end{itemize}
\textbf{Definition of 'derivative':}
Let $Z$ be a $p\times q$ matrix and $Y$ be a $m\times n$ matrix whose elements are differentiable functions of all elements in $Z$. Define $\frac{\partial Y}{\partial Z}$ to be a $pq\times mn$ matrix whose $i$th row is the mn-vector $\frac{\partial {\overline Y}}{\partial z_i}$, where $z_i$ is the i-th element in ${\overline Z}$. Obviously from the definition, $\frac{\partial Y}{\partial Z} = \frac{\partial {\overline Y}}{\partial {\overline Z}}$.
\end{frame}

\begin{frame}
\frametitle{Examples}
Suppose $Z = (z_1, z_2), Y = \left( {\begin{array}{*{20}{c}}
  y_1&y_2 \\
  y_3&y_4
\end{array}} \right)$ and $X = \left( {\begin{array}{*{20}{c}}
  x_1&x_2 \\
  x_3&x_4
\end{array}} \right)$. Then
\begin{eqnarray*}
% \nonumber to remove numbering (before each equation)
  {\overline Y} &=& (y_1, y_2, y_3, y_4) \\
  \\
  D_{{\overline Y}}&=& diag\{y_1, y_2, y_3, y_4\} \\
  \\
  X*Y &=& \left( {\begin{array}{*{20}{c}}
                x_1y_1&x_2y_2 \\
                x_3y_3&x_4y_4
            \end{array}} \right) \\
            \\
  Z\otimes Y &=& (z_1 Y, z_2Y) \\
             &=& \left( {\begin{array}{*{20}{c}}
                z_1y_1&z_1y_2  & z_2y_1&z_2y_2 \\
                z_1y_3&z_1y_4  & z_3y_3&z_2y_4
                \end{array}} \right) \\
\end{eqnarray*}
\end{frame}

\begin{frame}
\frametitle{Examples (cont'd)}
\begin{eqnarray*}
% \nonumber to remove numbering (before each equation)
    \frac{\partial Y}{\partial Z} &=&
   \left( {\begin{array}{*{20}{c}}
     \frac{\partial y_1}{\partial z_1}& \frac{\partial y_2}{\partial z_1}& \frac{\partial y_3}{\partial z_1}& \frac{\partial y_4}{\partial z_1} \\\\
     \frac{\partial y_1}{\partial z_2}& \frac{\partial y_2}{\partial z_2}& \frac{\partial y_3}{\partial z_2}& \frac{\partial y_4}{\partial z_2} \\
    \end{array}} \right) \\
\end{eqnarray*}
\end{frame}

\begin{frame}
\frametitle{Chain Rule}
\begin{theorem}
Suppose each element of a \textbf{vector} $Y=[Y_1, Y_2]$ is a differential function of all elements of a \textbf{vector} $Z$, where $Y_1$ contains all the mathematical variables which are independent to each other in $Y$ and $Y_2$ are functions of elements in $Y_1$. Furthermore, suppose that each element of a \textbf{vector} $X$ is a differentiable function of all elements of $Y_1$. Then
\begin{equation}\label{chainrule}
\frac{\partial X}{\partial Z} = \frac{\partial Y_1}{\partial Z} \frac{\partial X}{\partial Y_1}
\end{equation}
\end{theorem}
\end{frame}

\begin{frame}
\frametitle{Proof of Chain Rule}
Suppose $X$ is $1\times r$, $Y_1$ is $1\times s$ and $Z$ is $1\times t$. Then we have
\[\frac{\partial X}{\partial Z} =
\left( {\begin{array}{*{20}{c}}
\frac{\partial x_1}{\partial z_1} & \cdots & \frac{\partial x_r}{\partial z_1}  \\
\vdots & \ddots& \vdots \\
\frac{\partial x_1}{\partial z_t}& \cdots & \frac{\partial x_r}{\partial z_t}
\end{array}} \right)\]
and
\[\frac{\partial Y_1}{\partial Z} \frac{\partial X}{\partial Y_1} =
\left( {\begin{array}{*{20}{c}}
\frac{\partial y_1}{\partial z_1} & \cdots & \frac{\partial y_s}{\partial z_1}  \\
\vdots & \ddots& \vdots \\
\frac{\partial x_1}{\partial z_t}& \cdots & \frac{\partial y_s}{\partial z_t}
\end{array}} \right)
\left( {\begin{array}{*{20}{c}}
\frac{\partial x_1}{\partial y_1} & \cdots & \frac{\partial x_r}{\partial y_1}  \\
\vdots & \ddots& \vdots \\
\frac{\partial x_1}{\partial y_s}& \cdots & \frac{\partial x_r}{\partial y_s}
\end{array}} \right)
\]
The (i,j)th element in $\frac{\partial X}{\partial Z}$ is $\frac{\partial x_i}{\partial z_j}$ while the corresponding element in $\frac{\partial Y_1}{\partial Z} \frac{\partial X}{\partial Y_1}$ is $\sum \limits_{l=1}^s \frac{\partial y_l}{\partial z_i} \frac{\partial x_j}{\partial y_l}$. By the chain rule in scalar calculus, we verify Eq.\ref{chainrule}.
\end{frame}

\begin{frame}
\frametitle{Warning}
Generally speaking, the following equation is NOT true!
\[\frac{\partial X}{\partial Z} = \frac{\partial Y}{\partial Z} \frac{\partial X}{\partial Y} \]
A counterexample: $X=[v_1, v_1, v_2, v_2],$ $Y=[v_1, v_2, v_1, v_2]$ and $Z = [v_1, v_2, v_2, v_1]$. Now
\[ \frac{\partial X}{\partial Z} =
\left( {\begin{array}{*{20}{c}}
1& 1& 0& 0 \\
0& 0& 1& 1 \\
0& 0& 1& 1 \\
1& 1& 0& 0
\end{array}} \right)
\]
But $\frac{\partial Y}{\partial Z} \frac{\partial X}{\partial Y} = \left( {\begin{array}{*{20}{c}}
2& 2& 0& 0 \\
0& 0& 2& 2 \\
0& 0& 2& 2 \\
2& 2& 0& 0
\end{array}} \right)$
\end{frame}

\begin{frame}
\frametitle{Product Rule}
\begin{theorem}
Suppose each element of a $p\times r$ matrix $Y$ and an $r\times q$ matrix $X$ is a differentiable function of all elements of a $m\times n$ matrix $Z$. Then
\begin{equation}\label{productrule}
  \frac{\partial(YX)}{\partial Z} = \frac{\partial Y}{\partial Z} (I^p \otimes X) + \frac{\partial X}{\partial Z}(Y' \otimes I^q)
\end{equation}
\end{theorem}
\end{frame}

\begin{frame}
\frametitle{Proof of Product Rule}
$\frac{\partial(YX)}{\partial Z}$ is a $mn\times pq$ matrix. Its (i,j)th element corresponds to the i-th element in ${\overline Z}$, say $z_\beta$, and j-th element in $\overline {XY}$, say $(XY)_{(s,t)}$. $(XY)_{(s,t)}$ is the product of the s-th row of $Y$ and t-th column of $X$, or $(XY)_{(s,t)} = \sum \limits_{l=1}^r y_{sl}x_{lt}$. So
\[ \frac{\partial(YX)}{\partial Z} = [\frac{\partial \sum \limits_{l=1}^r y_{sl}x_{lt}}{\partial z_\beta}] = \sum\limits_{l=1}^r \frac{\partial y_{sl}}{\partial z_\beta}x_{lt} + \frac{\partial x_{lt}}{\partial z_\beta}y_{sl}\]


\end{frame}

\begin{frame}
\frametitle{Proof of Product Rule (cont'd)}
Now consider $\frac{\partial Y}{\partial Z}(I^p \otimes X)$. $\frac{\partial Y}{\partial Z}$ is a $mn\times pr$ matrix while $I^p \otimes X$ is a $pr\times pq$ matrix. The (i,j)th element of $\frac{\partial Y}{\partial Z}(I^p \otimes X)$ is the product of i-th row of $\frac{\partial Y}{\partial Z}$ and j-th column of $I^p \otimes X$. The i-th row of corresponds to the i-th element of $\overline {Z}$, which is $z_\beta$. Recall that $I^p \otimes X$ is in the form of
$ \left( {\begin{array}{*{20}{c}}
X & & & \\
 &X & & \\
 &  &\ddots& \\
 & & & X
\end{array}} \right)
$
So the j-th column of $I^p \otimes X$ corresponds to the t-column of $X$. And the t-th column of $X$ should be multiplied by the s-th row of $Y$. So the (i,j)th element of $\frac{\partial Y}{\partial Z}(I^p \otimes X)$ is $\sum \limits_{l=1}^k \frac{\partial y_{sl}}{\partial z_\beta} x_{lt}$. Similarly, we can prove that the (i,j)th element of $\frac{\partial X}{\partial Z}(Y' \otimes I^q)$ is $\sum \limits_{l=1}^k \frac{\partial x_{lt}}{\partial z_\beta}y_{sl}$. Add these two parts up is just the (i,j)th element of $\frac{\partial(YX)}{\partial Z}$.
\end{frame}
\begin{frame}
\frametitle{Properties of Hadamard Product Cont'd}
The Hadamard product is commutative, associative and distributive over addition. That is
\begin{eqnarray}
        % \nonumber to remove numbering (before each equation)
          A*B &=& B*A \\
          A*(B*C) &=& (A*B)*C \\
          A*(B+C) &=& A*B+A*C
        \end{eqnarray}
The identity matrix under Hadamard multiplication of two m-by-n matrices is m-by-n matrix where all elements are equal to 1, which is different from simple identity matrix $I$\\
A matrix has an inverse under Hadamard multiplication if and only if none of the elements are equal to zero.\\
In programming language like MATLAB, Hadamard product is done by using $.*$ where $*$ stands for the normal multiplication.
\end{frame}

\begin{frame}
\frametitle{Properties of Hadamard Product}
For square $A$ and $B$, the row-sums of their Hadamard product are the diagonal elements of $AB^{T}$
\begin{equation}
\sum_{j}(A*B)_{i,j} = (AB^{T})_{i,j}
\end{equation}
\\
The Hadamard product is a principal submatrix of the Kronecker product.\\
A principal submatrix is a square submatrix where the distinguished rows and columns are the same.
\end{frame}

\begin{frame}
\frametitle{Schur Product Theorem}
\begin{theorem}
The Hadamard product of two positive-semidefinite matrices is positive-semidefinite. This is known as the Schur product theorem
\end{theorem}
\begin{proof}
Proof is done using eigendecomposition.\\
Let $M = \sum u_{i}m_{i}m_{i}^{T} $ and $N = \sum v_{i}n_{i}n_{i}^{T}$. Then $M*N = \sum_{i,j} u_{i}v_{j}(m_{i}m_{i}^{T})*(n_{j}n_{j}^{T}) = \sum_{i,j} u_{i}v_{j}(m_{i}*n_{j})(m_{i}*n_{j})^{T}$.
Each $(m_{i}*n_{j})(m_{i}*n_{j})^{T}$ is positive and $u_{i}v_{j} >0$, thus the sum giving $M*N$ is also positive.
\end{proof}
\end{frame}


\begin{frame}
\frametitle{Product Rule for Hadamard Product}

\begin{theorem}
Suppose $Y$ and $X$ are $m-n$ matrices and each of whose elements is a differentiable function of all elements of a $p-n$ matrix Z. Then
\begin{equation}
\frac{\partial{(Y*X)}}{\partial{Z}} = \frac{\partial{Y}}{\partial{Z}}(D_{\hat{x}}) + \frac{\partial{X}}{\partial{Z}}(D_{\hat{y}}).
\end{equation}
\end{theorem}

\begin{proof}
\begin{equation}
\frac{\partial{Y}}{\partial{Z}}(D_{\hat{x}}) + \frac{\partial{X}}{\partial{Z}}(D_{\hat{y}}) = [\sum_{ij }(\frac{\partial{y_{ij}}}{\partial{z_{st}}}x_{ij})+\sum_{jk}(y_{jk}\frac{\partial{x_{jk}}}{\partial{z_{st}}})]
\end{equation}
By the definition of $D_{x}$,
\begin{equation}
RHS=\frac{\partial{(Y*X)}}{\partial{Z}}.
\end{equation}
\end{proof}

\end{frame}

\begin{frame}
\frametitle{Physical Meaning of Hadamard Product}
A very intuitive way to understand Hadamard product is to view it as a amplification of image processing.
Here is a demonstration of Hadamard product that reduces the brightness of a picture. The matrix of original picture Hadamard-multiplies another matrix of exponential decay from the center.

\begin{table}
  \centering
  \begin{tabular}{ l l l}

    % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
    \includegraphics[width=4cm]{Original.eps} & \includegraphics[width=4cm]{H.eps} & \includegraphics[width=4cm]{New.eps} \\

  \end{tabular}
  %\caption{}\label{}
\end{table}



\end{frame}

\begin{frame}
\frametitle{Hadamard Matrix}
The Hadamard matrix $H$ is a square with elements 1 or -1. It has several interesting properties.
\begin{equation}
HH^{T} = nI_{n}
\end{equation}
A Hadamard matrix has maximal determinant among matrices with entries of absolute value less than or equal to 1. If $M$ is a complex matrix of order n, whose entries are bounded by $M_{ij}\leq 1$, then $|det(M)| \leq n^{n/2}$.\\
The Hadamard matrix can be constructed by Sylvester's construction.
\end{frame}


\begin{frame}
\frametitle{Sylvester's construction}
If $H$ is a Hadamard matrix, then the matrix below is also a Hadamard matrix.
$
\left(
  \begin{array}{cc}
    H & H \\
    H & -H \\
  \end{array}
\right)
$
For example, $H_{1} = [1]$, $H_{2} = \left(
                                       \begin{array}{cc}
                                         1 & 1 \\
                                         1 & -1 \\
                                       \end{array}
                                     \right)$.
$H_{2^{k}} = \left(
               \begin{array}{cc}
                 H_{2^{k-1}} & H_{2^{k-1}} \\
                 H_{2^{k-1}} & H_{2^{k-1}} \\
               \end{array}
             \right) = H_{2} \otimes H_{2^{k-1}}$ where $\otimes$ denotes the Kronecker product.

\end{frame}

\end{document}                     %文档结束
